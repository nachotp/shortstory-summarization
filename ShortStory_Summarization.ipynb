{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShortStory-Summarization.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachotp/shortstory-summarization/blob/master/ShortStory_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DebJYC2WQX3I",
        "colab_type": "text"
      },
      "source": [
        "# Short Story Title Prediction using Seq2Seq with Attention Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deaw65IV1XY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-GW-5kVPc3h",
        "colab_type": "text"
      },
      "source": [
        "## Creación de embeddings Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfDQozpRsfY8",
        "colab_type": "code",
        "outputId": "fe3885d7-f63e-4c18-eeea-4226890fec8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stories = open(\"text8\")\n",
        "\n",
        "documents = []\n",
        "for st in stories:\n",
        "  documents.append(st.strip().split(\" \"))\n",
        "\n",
        "print(len(documents))\n",
        "\n",
        "model = gensim.models.Word2Vec(\n",
        "        documents,\n",
        "        size=150,\n",
        "        window=3,\n",
        "        min_count=5,\n",
        "        workers=10,\n",
        "        iter=10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnh-K6qTPgcg",
        "colab_type": "text"
      },
      "source": [
        "## Parsing de dataset\n",
        "\n",
        "### Carga de datos y segmentación de sentencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJgCuogsz0DM",
        "colab_type": "code",
        "outputId": "b360e794-9646-4560-fba8-22d99e835e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for line in open(\"data.csv\"):\n",
        "  target_text, input_text = line.strip().split(\";\")\n",
        "\n",
        "  #if len(target_text.split()) >= 2:\n",
        "  #  target_text = \" \".join(target_text.split()[:2])\n",
        "\n",
        "  if len(input_text.split(\" \")) > 20:\n",
        "    input_split = np.array_split(input_text.split(\" \"),np.ceil( len(input_text.split(\" \")) / 15))\n",
        "    for input_t in input_split:\n",
        "      input_texts.append(\" \".join(input_t))\n",
        "      target_texts.append(f\"start {target_text} end\")\n",
        "  else:\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(f\"start {target_text} end\")\n",
        "\n",
        "max_len_text = np.max([len(t.split(\" \")) for t in input_texts])\n",
        "max_len_summary = max([len(t.split(\" \")) for t in target_texts])\n",
        "\n",
        "print(max_len_summary, max_len_text)\n",
        "np.mean([len(t.split(\" \")) for t in input_texts])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.697495660798413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qvy-jhtPnlQ",
        "colab_type": "text"
      },
      "source": [
        "### Tokenización y padding de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXW1e9NZKVH",
        "colab_type": "code",
        "outputId": "27877662-2ab5-4d58-9965-08279e6dcca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val = train_test_split(input_texts, target_texts,test_size=0.15,shuffle=True) \n",
        "\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
        "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "\n",
        "x_voc_size   =  len(x_tokenizer.word_index) + 1\n",
        "print(x_voc_size)\n",
        "\n",
        "#preparing a tokenizer for summary on training data \n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr)+list(y_val))\n",
        "\n",
        "print(y_val[1])\n",
        "#convert summary sequences into integer sequences\n",
        "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post', value=0)\n",
        "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post', value=0)\n",
        "\n",
        "y_voc_size  =   len(y_tokenizer.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11373\n",
            "start ficción end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLC-4bMcPvrP",
        "colab_type": "text"
      },
      "source": [
        "## Modelo Seq2Seq\n",
        "\n",
        "### Capa de Atención custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmcj7nJeEw95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGXzGeZHP1Ym",
        "colab_type": "text"
      },
      "source": [
        "### Arquitectura Seq2Seq con Atención y LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lJJWyh421bK",
        "colab_type": "code",
        "outputId": "3802328c-6d2f-40e4-baed-2d963803c28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "\n",
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "latent_dim = 30\n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_text,)) \n",
        "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
        "\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True, dropout=0.3) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True, dropout=0.3) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True, dropout=0.3) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "#LSTM 3 \n",
        "#encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.3) \n",
        "#encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 30)       341190      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 30)     42090       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 20, 30), (No 7320        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 30), ( 7320        embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 30), ( 1830        lstm[0][0]                       \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 60)     0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 1403)   85583       concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 485,333\n",
            "Trainable params: 485,333\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snaxq-C922YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9I-bsxCE1nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikWLpQe4FHpy",
        "colab_type": "code",
        "outputId": "cc3e48b8-2f8a-43ed-cd12-36aca002a2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,batch_size=256, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n",
        "\n",
        "history=model.fit([x_tr,y_tr[:,:-1]],\n",
        "                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n",
        "                  epochs=50,\n",
        "                  #callbacks=[es],\n",
        "                  batch_size=128,\n",
        "                  validation_data=([x_val,y_val[:,:-1]],y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]),\n",
        "                  workers=10\n",
        "                  )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3428 samples, validate on 605 samples\n",
            "Epoch 1/50\n",
            "3428/3428 [==============================] - 4s 1ms/sample - loss: 6.1579 - val_loss: 4.0500\n",
            "Epoch 2/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 2.9477 - val_loss: 2.4238\n",
            "Epoch 3/50\n",
            "3428/3428 [==============================] - 2s 640us/sample - loss: 2.2852 - val_loss: 2.2696\n",
            "Epoch 4/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 2.1605 - val_loss: 2.1864\n",
            "Epoch 5/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 2.0686 - val_loss: 2.1020\n",
            "Epoch 6/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.9857 - val_loss: 2.0418\n",
            "Epoch 7/50\n",
            "3428/3428 [==============================] - 2s 648us/sample - loss: 1.9138 - val_loss: 1.9914\n",
            "Epoch 8/50\n",
            "3428/3428 [==============================] - 2s 647us/sample - loss: 1.8593 - val_loss: 1.9492\n",
            "Epoch 9/50\n",
            "3428/3428 [==============================] - 2s 638us/sample - loss: 1.8129 - val_loss: 1.9198\n",
            "Epoch 10/50\n",
            "3428/3428 [==============================] - 2s 636us/sample - loss: 1.7768 - val_loss: 1.9024\n",
            "Epoch 11/50\n",
            "3428/3428 [==============================] - 2s 649us/sample - loss: 1.7471 - val_loss: 1.8983\n",
            "Epoch 12/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.7217 - val_loss: 1.8712\n",
            "Epoch 13/50\n",
            "3428/3428 [==============================] - 2s 640us/sample - loss: 1.6985 - val_loss: 1.8631\n",
            "Epoch 14/50\n",
            "3428/3428 [==============================] - 2s 642us/sample - loss: 1.6810 - val_loss: 1.8565\n",
            "Epoch 15/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.6622 - val_loss: 1.8675\n",
            "Epoch 16/50\n",
            "3428/3428 [==============================] - 2s 644us/sample - loss: 1.6467 - val_loss: 1.8472\n",
            "Epoch 17/50\n",
            "3428/3428 [==============================] - 2s 639us/sample - loss: 1.6343 - val_loss: 1.8593\n",
            "Epoch 18/50\n",
            "3428/3428 [==============================] - 2s 648us/sample - loss: 1.6205 - val_loss: 1.8401\n",
            "Epoch 19/50\n",
            "3428/3428 [==============================] - 2s 646us/sample - loss: 1.6061 - val_loss: 1.8422\n",
            "Epoch 20/50\n",
            "3428/3428 [==============================] - 2s 638us/sample - loss: 1.5941 - val_loss: 1.8284\n",
            "Epoch 21/50\n",
            "3428/3428 [==============================] - 2s 643us/sample - loss: 1.5806 - val_loss: 1.8227\n",
            "Epoch 22/50\n",
            "3428/3428 [==============================] - 2s 650us/sample - loss: 1.5660 - val_loss: 1.8177\n",
            "Epoch 23/50\n",
            "3428/3428 [==============================] - 2s 643us/sample - loss: 1.5549 - val_loss: 1.8142\n",
            "Epoch 24/50\n",
            "3428/3428 [==============================] - 2s 646us/sample - loss: 1.5405 - val_loss: 1.8164\n",
            "Epoch 25/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 1.5249 - val_loss: 1.8082\n",
            "Epoch 26/50\n",
            "3428/3428 [==============================] - 2s 643us/sample - loss: 1.5132 - val_loss: 1.8077\n",
            "Epoch 27/50\n",
            "3428/3428 [==============================] - 2s 646us/sample - loss: 1.4994 - val_loss: 1.7973\n",
            "Epoch 28/50\n",
            "3428/3428 [==============================] - 2s 655us/sample - loss: 1.4867 - val_loss: 1.7904\n",
            "Epoch 29/50\n",
            "3428/3428 [==============================] - 2s 643us/sample - loss: 1.4728 - val_loss: 1.7897\n",
            "Epoch 30/50\n",
            "3428/3428 [==============================] - 2s 642us/sample - loss: 1.4588 - val_loss: 1.7860\n",
            "Epoch 31/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 1.4448 - val_loss: 1.7814\n",
            "Epoch 32/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 1.4320 - val_loss: 1.7739\n",
            "Epoch 33/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.4162 - val_loss: 1.7866\n",
            "Epoch 34/50\n",
            "3428/3428 [==============================] - 2s 639us/sample - loss: 1.4045 - val_loss: 1.7787\n",
            "Epoch 35/50\n",
            "3428/3428 [==============================] - 2s 637us/sample - loss: 1.3893 - val_loss: 1.7819\n",
            "Epoch 36/50\n",
            "3428/3428 [==============================] - 2s 640us/sample - loss: 1.3782 - val_loss: 1.7860\n",
            "Epoch 37/50\n",
            "3428/3428 [==============================] - 2s 649us/sample - loss: 1.3644 - val_loss: 1.7844\n",
            "Epoch 38/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.3523 - val_loss: 1.7777\n",
            "Epoch 39/50\n",
            "3428/3428 [==============================] - 2s 642us/sample - loss: 1.3388 - val_loss: 1.7833\n",
            "Epoch 40/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 1.3254 - val_loss: 1.7804\n",
            "Epoch 41/50\n",
            "3428/3428 [==============================] - 2s 649us/sample - loss: 1.3142 - val_loss: 1.7819\n",
            "Epoch 42/50\n",
            "3428/3428 [==============================] - 2s 641us/sample - loss: 1.3011 - val_loss: 1.7860\n",
            "Epoch 43/50\n",
            "3428/3428 [==============================] - 2s 647us/sample - loss: 1.2901 - val_loss: 1.7900\n",
            "Epoch 44/50\n",
            "3428/3428 [==============================] - 2s 648us/sample - loss: 1.2766 - val_loss: 1.7777\n",
            "Epoch 45/50\n",
            "3428/3428 [==============================] - 2s 643us/sample - loss: 1.2646 - val_loss: 1.7846\n",
            "Epoch 46/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.2535 - val_loss: 1.7902\n",
            "Epoch 47/50\n",
            "3428/3428 [==============================] - 2s 650us/sample - loss: 1.2419 - val_loss: 1.7773\n",
            "Epoch 48/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.2293 - val_loss: 1.7894\n",
            "Epoch 49/50\n",
            "3428/3428 [==============================] - 2s 638us/sample - loss: 1.2183 - val_loss: 1.7919\n",
            "Epoch 50/50\n",
            "3428/3428 [==============================] - 2s 645us/sample - loss: 1.2062 - val_loss: 1.7928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZzxVSpqP8hY",
        "colab_type": "text"
      },
      "source": [
        "### Creación del modelo predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGQl0UglQRID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CKGYVDlQafV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix6n_0DQRDXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import choice\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])# choice(np.argsort(output_tokens[0, -1, :],axis=-1)[:3])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'end' or len(decoded_sentence.split(\" \")) >= (max_len_summary-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbSzsP1_RH-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAqcivWfP_48",
        "colab_type": "text"
      },
      "source": [
        "### Pruebas de resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bocFkJ0RKgM",
        "colab_type": "code",
        "outputId": "b6e050a1-9636-4a60-9d7b-592497b0f4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"Review:\",seq2text(x_val[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_val[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_len_text)))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: noche cuarto puede hacerlo momento cualquier pesar cuarto noche victor \n",
            "Original summary: epistolar \n",
            "Predicted summary:  mujer\n",
            "\n",
            "\n",
            "Review: 15 septiembre 2018 lluvia parque local amistad beso \n",
            "Original summary: ficción \n",
            "Predicted summary:  amor amor\n",
            "\n",
            "\n",
            "Review: miraba escena sonrisa labios alejó bien amo preguntó gato negro andaba lado aburrido \n",
            "Original summary: idiota mano gorila \n",
            "Predicted summary:  nuevo capitales microcuentos amos\n",
            "\n",
            "\n",
            "Review: septiembre 2018 puedes leer microcuentos participantes momento reto invito participar dejar microrrelato comentario \n",
            "Original summary: pecado original \n",
            "Predicted summary:  sueño vida\n",
            "\n",
            "\n",
            "Review: día policía recibió medalla sintió superpoder autoridad medida calles uniforme medalla \n",
            "Original summary: superpoder autoridad \n",
            "Predicted summary:  fuerte\n",
            "\n",
            "\n",
            "Review: sueño “ si quiero hadas inviten país creer siempre ” decía pequeña caer profundamente dormida \n",
            "Original summary: hadas abuela \n",
            "Predicted summary:  bruja solterona\n",
            "\n",
            "\n",
            "Review: 13 octubre 2018 despiertes hospital monstruo dentro mal lado hermana \n",
            "Original summary: epistolar \n",
            "Predicted summary:  amor amor\n",
            "\n",
            "\n",
            "Review: enfurecido sacó rifle primer animal cruzó frente propio cuerpo jabalí morir alma \n",
            "Original summary: cadena alimenticia \n",
            "Predicted summary:  sueño vida\n",
            "\n",
            "\n",
            "Review: vamos solo horita vamos pasar tan rico apuesto va ir volando susurró gordo \n",
            "Original summary: tiempo relativo \n",
            "Predicted summary:  gran lago rayo\n",
            "\n",
            "\n",
            "Review: resultados rindió allí así adulto intentó tocar batería bajo violín solo terminó sufriendo \n",
            "Original summary: sueño inmortal \n",
            "Predicted summary:  pescador pez\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}